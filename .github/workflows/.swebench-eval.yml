name: SWE-bench Pro Evaluation (Gemini)

on:
  workflow_dispatch:
    inputs:
      task_id:
        description: 'Task ID to run'
        required: true
        default: 'internetarchive__openlibrary-c4eebe6677acc4629cb541a98d5e91311444f5d4'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    container:
      image: manojva/openlibrary-python312:latest
      options: --user root

    env:
      TASK_ID: ${{ github.event.inputs.task_id }}

    steps:
      - name: Checkout evaluation repository
        uses: actions/checkout@v4

      - name: Prepare directories
        run: |
          mkdir -p /workspace/artifacts
          mkdir -p /testbed
      - name: Setup OpenLibrary repository
        shell: bash
        run: |
          echo "--- Starting Inline Repository Setup ---"
          # Extract commit hash from TASK_ID
          COMMIT_HASH=$(echo "$TASK_ID" | rev | cut -d'-' -f1 | rev)
          echo "Target Commit: $COMMIT_HASH"
          
          if [ ! -d "/testbed/.git" ]; then
            git clone https://github.com/internetarchive/openlibrary.git /testbed
          fi
          
          cd /testbed
          git checkout "$COMMIT_HASH"

          # 1. Install system dependencies (required for many Python C-extensions)
          apt-get update && apt-get install -y libpq-dev build-essential

          # 2. Pre-install build dependencies in the main environment
          python -m pip install --upgrade pip
          pip install Cython setuptools wheel

          # 3. Install OpenLibrary using the system environment (no isolation)
          # This allows the setup script to see the Cython we just installed.
          echo "Installing OpenLibrary in editable mode..."
          pip install --no-build-isolation -e .
          
          echo "--- Setup Complete ---" | tee /workspace/artifacts/setup.log
    
      - name: Pre-verification test (expected to fail)
        shell: bash
        run: |
          set +e
          cd /testbed
          python -m pytest \
            openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending \
            -xvs | tee /workspace/artifacts/pre_verification.log
          echo $? > /workspace/artifacts/pre_verification.exit
          exit 0

      - name: Assert pre-verification failed
        shell: bash
        run: |
          EXIT_CODE=$(cat /workspace/artifacts/pre_verification.exit)
          if [ "$EXIT_CODE" -ne 0 ]; then
            echo "✅ Pre-verification failed as expected"
          else
            echo "❌ Pre-verification unexpectedly passed"
            exit 1
          fi

      - name: Install Google Generative AI SDK
        run: pip install google-generativeai

      - name: Run Gemini agent
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python scripts/run_agent.py \
            --task-id "$TASK_ID" \
            --repo-path /testbed \
            --log-path /workspace/artifacts/agent.log \
            --prompt-log /workspace/artifacts/prompts.md

      - name: Capture code changes
        if: always()
        shell: bash
        run: |
          cd /testbed
          git diff > /workspace/artifacts/changes.patch || true

      - name: Post-verification test
        shell: bash
        run: |
          cd /testbed
          python -m pytest \
            openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending \
            -xvs | tee /workspace/artifacts/post_verification.log

      - name: Extract evaluation metrics
        if: always()
        shell: bash
        run: |
          python -c "
          import json, os
          log_path = '/workspace/artifacts/agent.log'
          metrics = {'status': 'failed', 'iterations': 0}
          if os.path.exists(log_path):
              with open(log_path) as f:
                  content = f.read()
                  metrics['iterations'] = content.count('--- Iteration')
                  if 'Fix successful' in content: metrics['status'] = 'passed'
          with open('/workspace/artifacts/result.json', 'w') as f:
              json.dump(metrics, f, indent=2)
          "

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: swebench-eval-artifacts
          path: /workspace/artifacts
