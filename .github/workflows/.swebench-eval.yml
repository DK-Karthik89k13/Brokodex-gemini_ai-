name: SWE-bench Pro Evaluation (Gemini)

on:
  workflow_dispatch:
    inputs:
      task_id:
        description: "Task ID to run"
        required: true
        default: "internetarchive__openlibrary-c4eebe6677acc4629cb541a98d5e91311444f5d4"

jobs:
  evaluate:
    runs-on: ubuntu-latest
    container:
      image: manojva/openlibrary-python312:latest
      options: --user root

    env:
      TASK_ID: ${{ github.event.inputs.task_id }}
      PYTHONPATH: /testbed
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      GEMINI_MODEL: gemini-1.5-pro

    steps:
      - name: Checkout evaluator repo
        uses: actions/checkout@v4

      - name: Prepare directories
        run: |
          mkdir -p /workspace/artifacts
          mkdir -p /testbed

      - name: Checkout OpenLibrary at task commit
        shell: bash
        run: |
          set -e
          COMMIT_HASH=$(echo "$TASK_ID" | rev | cut -d'-' -f1 | rev)
          if [ ! -d /testbed/.git ]; then
            git clone https://github.com/internetarchive/openlibrary.git /testbed
          fi
          cd /testbed
          git fetch --all
          git checkout "$COMMIT_HASH"

      - name: Install Python dependencies
        shell: bash
        run: |
          cd /testbed
          python -m pip install --upgrade pip
          pip install web.py pytest Cython setuptools wheel google-genai google-generativeai

      - name: Install infogami (idempotent)
        shell: bash
        run: |
          cd /testbed
          if [ ! -d infogami ]; then
            git clone https://github.com/internetarchive/infogami.git
          fi
          pip install -e infogami

      - name: Sanity check infogami import
        shell: bash
        run: |
          python - << 'EOF'
          import infogami
          from infogami.infobase.tests.pytest_wildcard import Wildcard
          print("infogami import OK")
          EOF

      - name: Pre-verification test (expected to fail)
        shell: bash
        run: |
          cd /testbed
          set +e
          python -m pytest \
            openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending \
            -xvs | tee /workspace/artifacts/pre_verification.log
          echo $? > /workspace/artifacts/pre_verification.exit

      - name: Assert pre-verification failed
        shell: bash
        run: |
          [ "$(cat /workspace/artifacts/pre_verification.exit)" -ne 0 ]

      - name: Run Gemini agent
        shell: bash
        run: |
          python scripts/run_agent.py \
            --task-id "$TASK_ID" \
            --repo-path /testbed \
            --log-path /workspace/artifacts/agent.log \
            --prompt-log /workspace/artifacts/prompts.md \
            --pre-log /workspace/artifacts/pre_verification.log \
            --results /workspace/artifacts/results.json \
            --model "$GEMINI_MODEL"

      - name: Capture code changes
        if: always()
        shell: bash
        run: |
          cd /testbed
          git diff > /workspace/artifacts/changes.patch || true

      - name: Post-verification test
        shell: bash
        run: |
          cd /testbed
          python -m pytest \
            openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending \
            -xvs | tee /workspace/artifacts/post_verification.log

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: swebench-eval-artifacts
          path: /workspace/artifacts
